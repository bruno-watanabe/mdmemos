# コンペ概要

## コンペ説明
RMSタイタニック号の沈没は歴史上最も悪名高い難破船のひとつです。
1912年4月15日、乙女の航海中、タイタニック号は氷山と衝突した後沈没し、2224人の乗客と乗組員のうち1502人が死亡した。
このセンセーショナルな悲劇は国際社会に衝撃を与え、船舶の安全規制を改善することにつながりました。
難破船がこのような命の喪失につながった理由の1つは、乗客と乗組員のための救命ボートが十分になかったことです。
沈没を乗り切ることには幸運の要素が含まれていましたが、女性、子供、そして上流階級のような人々のグループは、他の人々よりも生き残る可能性がありました。
この課題では、どのような種類の人々が生き残る可能性が高いかの分析を完了するようお願いします。特に、どの乗客が悲劇を乗り越えたかを予測するために、機械学習のツールを適用するようお願いします。

考察:
- コンペ目的はどのような条件の人が生き残る確率が高いか分類
- 救命ボートが十分になかった
- 女性、子供が優先されていた
- 上流階級の人も優先されていた

## 提出形式
- PassengerId (sorted in any order)
- Survived (contains your binary predictions: 1 for survived, 0 for deceased)
上記2列のcsvファイル

## Data
- traning.csv
- test.csv
- gender_submission.csv->女性のみが生き残ったと仮定したデータ

|Variable	|Definition	|Key
| --- | --- | ---|
|survival	|Survival	|0 = No, 1 = Yes
|pclass	|Ticket class	|1 = 1st, 2 = 2nd, 3 = 3rd
|sex	|Sex	|
|Age	|Age in years	|
|sibsp	|# of siblings / spouses aboard the Titanic	|
|parch	|# of parents / children aboard the Titanic	|
|ticket	|Ticket number	|
|fare	|Passenger fare	|
|cabin	|Cabin number	|
|embarked	|Port of Embarkation	|C = Cherbourg, Q = Queenstown, S = Southampton

# Kernels1
# EDA To Prediction(DieTanic)
1. 探索的データ分析
    1. 特徴分析
    2. 特徴の関係、傾向の確認　
2. 特徴エンジニアリングとデータ整形
    1. 新しい特徴の生成
    2. 冗長な特徴の削除
    3. 特徴のデータ変換
3. 予測モデリング
    1. 基本アルゴリズムの実行
    2. Cross Validation
    3. アセンブル
    4. 重要な特徴の抽出

## 1. 探索的データ分析
- 欠損値確認->data.isnull().sum()
- 目的変数のカテゴリの割合を確認->生き残った割合は38.4%
- 説明変数を大まかに分類
    - 名義
    - 順序
    - 連続値
- 説明変数と目的変数の関係を確認
```
考察:
女性のほうが男性より母数が少ないが生存数は多いつまり生存率が高い

乗客の階級(1,2,3 <-1がもっとも上)は階級が高いほど母数が少ないが生存数は高いつまり階級が高いほど生存率が高い

性別と階級の関係性を見ると階級1〜2の女性の生存率が極端に高い
また、階級2〜3の男性の生存率は低い
階級3の女性と階級1の男性の生存率は近い値となっている
上記結果より女性が階級より優先的に救助されたと考えられる

年齢での生存率は10代が相対的に良い
20〜50代の女性が特に高い
男性は年齢が高いほど生存率が低い
```
- 欠損値の穴埋め
    - 年齢が欠損値多め
    - 全欠損値に平均値埋めだと実際との乖離が大きくなるのでは?
    - 平均のカテゴリ分けを行い乖離を少しでも少なくできる方法が必要
    - 名前の呼称(Mr,Mrs)が判別対象にできそう
    - 文字抽出は正規表現で行う
    - 抽出後結果確認
    - 複数カテゴリになったら内包できるカテゴリに置換してカテゴリ数を削減
    - それぞれのカテゴリの年齢平均で欠損値穴埋め
- 生存の有無を年齢をX軸に頻出数をプロット

```
考察:
1. 5歳未満の生存率高い
2. 最も高齢は80歳
3. 死亡者数のモードは30-40代
4. 女性子供は階級関係なく生存率が高い
```
- 乗船場所での生存率
    - 乗船場と女性男性での生存数非生存数を確認
    - Cポートが相対的に生存率が高い
    - 乗船場のカテゴリ毎の数-女性男性-生存数-階級それぞれでヒストグラムと生存率をY軸に乗船場-階級-男性女性をプロット
```
考察:
1. Sポートは3階級が多数を占めていた
2. Cポートは1-2階級が多かったため優先的に救助されていた
3. Qポートのほとんどは3階級だった
4. 1-2階級の女性はどのカテゴリでも生存率が高い
5. Sポートは相対的に生存率が低い
6. Qポートの男性たちの生存率は極端に低い
```
※大多数の乗客はSポートだったため欠損値はSで埋める

- 乗客の配偶者/兄弟連れ数
    - 配偶者/兄弟連れ数と生存数(合計と分散)でプロット
    - 配偶者/兄弟連れと階級の数確認
```
考察:
1. 独り身は34.5%ほどの生存率
2. 配偶者/兄弟数が増えるほど生存率は下がる傾向->理由は自分より配偶者/兄弟の救助を優先した可能性があるため
3. 5〜人から配偶者/兄弟連れは生存率が0%->理由はこの数の配偶者/兄弟連れは階級が3であったため
```
- 乗客の子持ち数
    - 3階級が多い傾向

``` 
両親が乗っている乗客は生存率が高い
1〜3人が生存率が高い
4〜は低い
```
- 運賃
    - 階級と正の相関が高い

- 各特徴の総括
```
sex - 女性の生存率が高い
Pclass - 階級が高いほど生存率が高い
Age - 5-10未満は生存率高い,15-35が最も生存率が低い
Embarked - Sに乗客が最も乗っており生存率が相対的に低い Cには1階級が多くいたため生存率が高い Qは3階級が多くいたため生存率が低い
Parch+SibSp - 一人また大家族は少数
```

- 各特徴に対する相関
    - 相関が高い特徴は共線性が高くなるため削除

## 2. 特徴エンジニアリングとデータ整形
```
目的:
特徴量の冗長性の排除
特徴量の合成
```

- 年齢幅
    - 年齢は連続値となっているためグループ分けをする必要あり
    - 最大値からグループ数を割ることで間隔の閾値を作成
- 家族数
    - 家族持ちか独り身かを大分できる
    - 独り身は相対的に生存率が低い
- 運賃
    - 連続値のため年齢と同様の処理を行う
    - 高いほど生存率も高い
- 性別、乗船場、呼称のカテゴリ化
- 使わない特徴の削除
- 相関を確認

## 3. 予測モデリング
主な部類アルゴリズム
1. Logistic Regression
2. Support Vector Machines(Linear and radial)
3. Random Forest
4. K-Nearest Neighbours
5. Naive Bayes
6. Decision Tree
7. Logistic Regression

- 上記アルゴリズムの精度が分類器の決定にはならない
    - テストデータを当てたときに同様の精度が得られると限らないため
- Cross validationで精度の確認
    - トレーニングデータを分割する
    - 分割した一つを検証データとする
    - 検証データ以外で学習して検証データで精度確認
    - これを分割数分繰り返す
- 混同行列
    - モデルの誤分類を俯瞰するのに使える
    - 偽陽性と真陽性;偽陰性と真陰性をの絶対値が確認できる
    - 各分類器が擬陽性が高いか偽陰性が高いを確認してどちらを許容してよいか確認する
- ハイパーパラメーター
    - 学習モデルのパラメータを変更することで学習率を上げることが可能
    - 最後の微調整的役割
- Ensembling
    - 学習モデルを複数使って学習する
    - モデルの安定性向上に使える
    1)Voting Classifier
    2)Bagging
    3)Boosting

    -  Voting Classifier
        - 複数の学習モデルの結果の最も良いものを採用
    - Banking
        
    - Boosting
        - モデルの精度がサチるまでパラメータを更新する
- Ensembling後の混同行列確認
- 特徴量の寄与度確認
```
考察:
1. 重要な特徴量はInitial, Pclass, Family_Size, Fare_cat
2. Sexはそれほど重要な特徴量ではない
3. SexとInitialは正の相関が高い
```
# Kernels2
# Introduction to Ensembling/Stacking in Python
## スタッキング
```
初段の学習器の出力結果を次段の入力結果
```

## 学習・予測するスクリプトのクラス化
- 冗長的な部分をクラスで覆いコード量削減を狙う

## Cross Validation
- ベースモデルの学習にCross Validationを使用することでスタッキング時の過学習を防ぐ

## ベースモデルを定義
第一のベースモデルとして、５つのモデルを準備します。
1. Random Forest classifier
2. Extra Trees classifier
3. AdaBoost classifer
4. Gradient Boosting classifer
5. Support Vector Machine

## 特徴量寄与度をプロット
- 各ベースモデルをプロット
- ベースモデルの平均をとってプロット

## スタッキング
- ベースモデルの学習で調整したトレーニングデータとテストデータをXGBoostで使うことでXGBoostの予測精度を上げる


