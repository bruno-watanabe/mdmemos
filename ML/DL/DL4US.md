# 機械学習の目的と分類
- 機械学習 (machine learning)は、人工知能研究の1分野として「人間の学習能力や認知・判別能力を計算機上で再現する」ことを目的としています。

- この大目標の下、データの扱い方によって機械学習は次の3つに分類されます。

  - 教師あり学習 (supervised learning)
    - データは入力と出力からなり、中に潜む入出力関係をモデル化して捉えることで、新しい 入力に対して出力を予測することができます。
  - 出力が離散値なら分類、連続値なら回帰と区別することもあります。
    - 教師なし学習 (unsupervised learning)
  データには入出力の区別はなく、データ自身の生成構造や分布をモデル化して捉えることで、データの性質を知る上での参考となります。
  - 強化学習 (reinforcement learning)
    - データは状態・行動・報酬からなり、これらの対応関係をモデル化して捉えることで、報酬を最大化するような行動を特定できます。
    
# 機械学習のプロセス
機械学習を実際に行う際のプロセスを大枠でとらえると、

- 使用するモデルを仮定（モデル構築、モデルパラメータの設定）
    パターンを捉えるための"型"がモデルです。よってモデルの良し悪し（データの持つ構造に対して適切か）が結果に大きく影響します。
- モデルがパターンを見つけられるように学習させる（学習）
    モデルにデータを流し込み、パターンを抽出します。
- モデルの性能評価や利用
    様々なモデルから最も"良い"モデルを選択したいので、性能評価を行います。またモデルを利用して予測や次元削減などを行います。

# 機械学習モデルと深層学習
- 統計モデル
- 人間の脳の構造に着目したモデルといった神経生理学的な発想によるモデル
  - 代表例:多層パーセプトロン(Multi Layer Perceptron; MLP)
  - 補足;深層学習 (deep leaning)とは、この隠れ層（中間層）を複数に増やし、全体として3層以上の多層にしたMLPモデルを用いた機械学習のことを指しています。

# MLP (深層学習) の特徴
- MLPの最大の特徴はモデル構築/調整の自由度です

# ライブラリ
- TensorFlow、PyTorch、Chainer、Caffe

# MLPにおける教師あり学習
1. モデル構築（どんな層をどこに配置するかを決定）
MLPの場合は全体の層数や各層の機能・規模を決めることが対応します。
2. 学習
    - 用意した入出力データを用いて、モデルが同じデータを再現するように訓練すること
    - 訓練 = 「同じ入力に対する出力データとモデルの出力の"ズレ度合い"を下げる」
    - ズレ度合いを評価する関数のことを損失関数(loss function)
    - 最小化問題は解析的に解けないので、反復的な最適化を行う
    - 最適化手法 (optimization method)の選択は、損失関数最小化がどれだけうまくいくかを左右し、モデルの学習時間や性能にも大きな影響を与えます
    - データセット全体を使って1回最適化を行うという単位をエポック (epoch)と呼びます
    - 最適化計算を何エポックか回し、損失関数の最小化を行うのが学習ということです。
3. 性能評価や予測
    - 損失関数の値そのものが1つの評価指標
    - 学習データ以外の未知のデータに対する予測力を汎化性能 (generalization performance)と言います
    - 汎化性能が高いか否かがモデルの良し悪しを決めるということであり、**機械学習において最重要視すべき観点**
    - 学習用のデータセットとは別にテスト用（検証用）データセットをあらかじめ用意しておき、学習後にテスト用（検証用）データセットに対する予測精度を評価する
    - 予測精度の指標は正解率 (accuracy)、つまり（正しく分類できたデータ数）/（全データ数）
    - 正解率だけ高くても良いモデルとは限らない
        - 真陽性:TP,偽陽性:FP,真陰性:TN,偽陰性:FN
        - 偽陽性率＝𝐹𝑃 / (𝐹𝑃+𝑇𝑁)、真陽性率＝𝑇𝑃 / (𝑇𝑃+𝐹𝑁)
        - 偽陰性率＝𝐹𝑁 / (𝑇𝑃+𝐹𝑁)、真陰性率＝𝑇𝑁 / (𝐹𝑃+𝑇𝑁)
        - 正解率 (accuracy)＝𝑇𝑃+𝑇𝑁 / (𝑇𝑃+𝐹𝑁+𝐹𝑃+𝑇𝑁)
        - 適合率 (precision)＝𝑇𝑃 / (𝑇𝑃+𝐹𝑃)
        - 再現率 (recall)＝𝑇𝑃 / (𝑇𝑃+𝐹𝑁)
        - F-値 (F-measure)＝2 /( 1 / precision+1 / recall)：適合率と再現率の調和平均
    - データセットと学習
      - バッチ学習（一括学習、batch learning）
        - 概要:データセット全体を一度に全て使用してパラメータを決定
        - 特徴
          - バッチ学習ではスカラー計算ではなくベクトル・行列計算を行うので、SIMDやGPU等により高速化されやすい         
      - オンライン学習（逐次学習、online learning）
          - 概要:データセット全体を持っているが、学習を1データ毎に繰り返す
          - 特徴
            - オンライン学習では1度の学習に使用するデータが1つなので、省メモリ
      - ミニバッチ (mini-batch)
        - 概要:データセットを少数のデータ集合、つまりミニバッチに分割し、このミニバッチごとに学習を行う
        - 特徴
          - バッチ学習とオンライン学習のいいとこ取り
          - ミニバッチの利用により、使用メモリ量を抑えつつ、高速なベクトル・行列計算能力を利用することができます
# 手書き文字認識をしよう（ニューラルネットワーク入門）
## 解説
- データセットの中身
  - x:手書き数字画像(28×28)
  - y:正解のラベル（xの画像が表す数字）
- [Kerasから使えるデータ・セット一覧](https://keras.io/ja/datasets/)
## 損失関数
- 連続値のとき
  - 平均二乗誤差
- 離散値
  - (多クラス)交差エントロピー
## 評価関数
- モデルの出力の良し悪しを評価
- その時点でのモデルの評価指標を出力するのみ
- 使用することが多いのはaccuracy(正解率)
##  Functional API
- 複雑なモデルの構築
  - Inputレイヤーから構築を始める
  - 各レイヤーの返り値（テンソル）を次のレイヤーの入力として順々に構築していく
  - keras.models.Modelクラスに入力と出力を指定することでインスタンス化
## 前処理
- スケーリング
  - 特定の範囲にデータが収まるように変換する
  - 正規化
    - 平均や分散を特定の値にする
  - 白色化
    - データの各要素間の相関を取り除く
  - バッチ正規化
    - データ全体を考慮した
## 勾配に関するテクニック
- 勾配
  - パラメータの更新量
- 最適化アルゴリズム (optimizer)
  - Basic:
    - 最急降下法
      - 1階の微分である勾配のみを用いて最適化を行います。
      - ∇E(𝑤(𝑡)) ：パラメータの空間で損失関数が最も増加する方向（これを勾配といいます）
      - 𝜂 ：パラメータの空間で損失関数が最も減少する方向 (−∇E(𝑤(𝑡))) にどれだけ進むかの比率
    - 確率的勾配降下法(Stochastic Gradient Descent, SGD)
      - 各更新毎にランダムにミニバッチを構成し、その平均損失を使用する
      - lr: 学習率、0以上の実数
      - momentum: モーメンタム、0以上の実数（前回のパラメータ更新量を反映させる比率（3.2.2で説明））
      - decay: 更新毎の学習率の減衰率、0以上の実数
      - nesterov: Nesterov momentumを適用するかどうか（Trueならモーメンタム項の計算を1ステップ先読みして評価します（3.2.2で説明））
- 活性化関数 (activation)
  - 勾配の大きさについて勾配消失問題という課題が知られており、活性化関数の工夫はその対処法の1つとなっています。
  - 誤差逆伝播法という計算方法の特徴上おきてしまう
    - 誤差逆伝播方:（ある層の勾配）＝（1層前の勾配）×（2層前の勾配）×・・・×（出力層の勾配）と積の形で勾配を求める
    - 途中の勾配が小さいと入力層付近の勾配はどんどん0に近づいていってしまい勾配消失が発生する
    - 多層な場合ほど起きやすい
  - sigmoidよりもtanh、reluのほうがより大きな値をとり、勾配消失しにくい
  - 最近の論文でもreluもしくはその派生形を用いているものが多い
- 初期化 (initializer)
  - 各層のパラメータは0を中心とした乱数で初期化しますが、大きすぎる値で初期化すれば学習の初期段階での勾配が過大になり、逆に小さすぎる値だと勾配自体も過小になってしまい、いずれにしても学習はうまく進みません。
  - その値のスケール（分散）を適切に設定する必要
  - LeCunの初期化
  - Glorotの初期化（Xavierの初期化）
  - Heの初期化

## 過学習に対するテクニック
- 過学習
  - 学習がある程度以上進むと、次第に既知データが持つ、それ自身には意味がないような統計的なばらつきまで学習してしまい、未知データへの予測精度が落ちるという現象
- 正則化
  - 学習過程でいくつかのパラメータが自動的に機能しなくなると良いわけですが、これを実現する
  - 損失関数にパラメータの大きさに対するペナルティ項（正則化項）を含めます。
  - なるべく少ないパラメータでデータにフィットするようにできます。
  - 主に次のL2,L1正則化またそれらを組み合わせたElasticNetが用いられます。
  - L2正則化
    - 全パラメータの2乗和を正則化項として損失関数に加えます。
  - L1正則化
    - 全パラメータの絶対値の和を正則化項として損失関数に加えます。
    - L1正則化ではL2正則化よりもパラメータが0になりやすいという特徴（スパース性）があります。
  - ElasticNet
    - L1正則化とL2正則化の組み合わせ
  - 注意点
    - 実際に学習したモデルのコストが正則化項を含めない値が他のモデルに比べて減っているかを見る必要があります。
    - 正則化項はあくまで学習の都合上導入されたもので、予測の意味では、正則化項を含めない目的関数で評価すべきであるためです。
- 早期終了 (early stopping)
  - 早々に学習を止めてしまうことで過学習を回避する手
  - 早くに学習を止めることによって学習量過多による過学習を防げる
  - いつ止める
    - 検証データの誤差が大きくなってきた（或いは評価関数値が下がってきた）ところで学習をストップさせます。
- ドロップアウト (dropout)
  - 近似的にアンサンブル法を実現するものになっています。
  - 入力の一部をランダムに0にして出力するlayerの一種



# 畳み込みニューラルネットワーク (CNN)
## 解説
- CNN基礎
  - 画像認識、音声認識、自然言語処理などにおいて幅広く使用されていま
  - 構成
    - Convolution(畳み込み)層 と Pooling(プーリング)層 と呼ばれる層を組合させて構築
  - キーワード
    - 局所結合・重み共有による パラメータ削減
    - 不変性
  - 局所結合によるパラメータ削減
    - CNNの畳み込み層では、近い画素同士の結合のみを考えることでパラメータを削減しています。
    - CNNの畳み込み層では出力のユニットは 入力のある一定範囲に対してのみ結合 を持ちます。
    - 畳み込み層では画素同士の位置情報を保持したまま扱うことで、結合を疎にすることを可能にしています。
  - 重み共有によるパラメータ削減
    - 畳み込み層では1つの重み(黒い矢印)はすべての場所で使われます。
    - 畳み込み層ではどこにあるかという情報を写像しなにがあるかのみを残すことで、パラメータを大幅に削減することに成功しています。入力画像が大きくなった場合でもパラメータ数は増えないため、この効果はとても大きい
      - ただし、どこにあるかという情報が重要な場合は画像内の位置によって異なるパラメータを使用する場合があります。
  - 不変性
    - さらに獲得した特徴の歪みやずれにたいしての頑強性をあげるため、小さな領域での統計量 (Max、Mean) などを取ります。
    - プーリング層に対応する
  - ネットワークの構成
    - 畳み込み層->プーリング層->畳み込み層->プーリング層->...
    - 全結合層は位置情報を失うため、ネットワークの最後でのみ使います。
    - 最初の方の層では局所的な特徴 (エッジなど) を抽出し、層が進むにつれて大局的な特徴 (矩形など) を抽出することができます。
    -  **階層的な概念の抽出**が深層学習の大きな特徴
 - Convolution(畳み込み)層
  - 畳み込みの考え方
    - 畳み込み層における畳込みとは、入力にたいしてフィルターを掛けた (畳み込んだ) ときに得られる値のことです。
  - 2次元入力に対する畳込み
    - フィルターは一つの特徴に対応する
    - 複数のフィルターを設定することにより複数の特徴を獲得していると考えることができます。
    - フィルターのサイズを大きくすると広い範囲の特徴を、小さくすると小さな範囲の特徴を獲得することができます。
  - 3次元入力に対する畳込み
    - CNNでの各層の入力は実際には(縦のピクセル数)x(横のピクセル数)x(フィルター数)の3階テンソルとなります。 それに合わせてフィルターも3階テンソルとなりますが、畳み込みの考え方自体は同じです。
  - パラメータ削減の例
    - 全結合層では入力画像のサイズに比例してパラメータ数が増えるのに対し畳み込み層では増えないので、パラメータ削減の効果は入力画像が大きくなるにつれて大きくなります。
  - 出力のサイズ
    - 入力の縦or横の次元数を 𝑁 、フィルタの縦or横の次元数を 𝐹 、フィルタを動かす幅を 𝑆 とする
  - パディング
    - 特徴マップが縮小してしまうのを防ぐために、入力の両端に対して0などの値をくっつけることをします。
    - パディングにより端の方のユニットに対する畳込みの回数が増えるため、端の方に重要な情報がある場合には有効だと考えることができます。
    - Valid
      - 何もくっつけないパディング
    - Same
      - 入力と出力のサイズが変わらないようにするパディング
- Pooling(プーリング)層
  - プーリングの考え方
    - 小さな領域に対して統計量 (Max、Mean) を取ることで、位置のズレなどに対して頑強な特徴抽出を行います。
## テクニック・発展内容
- Data Augmentation
  - データ拡張:人工的な操作で回転反転等をおこないデータを水増しする
- 画像データの正規化
  - ノイズなどがあり画像間やピクセル間によって入力の値の分布が異なります。
  - 正規化することで入力層において特徴を抽出しやすくします。
  - Global Contrast Normalization (GCN)
    - 画像ごとにピクセルの値を平均0、分散1に正規化します。
  - Zero-phase Component Analysis (ZCA) Whitening
    - 入力の各要素間の相関をゼロ(白色化)にします。
    - PCAを利用して共分散行列を単位行列化 (分散1、共分散0) したのち、元の空間に戻します。
- Batch Normalization
  - 損失関数・勾配の形状が緩やかになり、勾配法などによる最適化が容易になることが実験的に・一部理論的に示されています
    - 各層の出力を正規化することでこの課題の解決を試みます。
  - ↓実は効果なし
    - 各隠れ層の入力の分布も安定させたい
    - 内部共変量シフト
      - 深層学習の課題として学習データと検証(評価)データ間で各層の分布が変わってしまう現象がある
- Skip Connection (Residual Network)
  - 層を飛び越えた結合をつくることで勾配消失問題を解消しようとする手法
- 学習済みネットワークの利用
  - あらかじめ別の大規模なデータセットで十分に学習されたネットワークの出力層以外の重みを初期値として活用することを考えます。
- 学習させたモデルの保存・再利用
  - 保存
  - 再利用

# 系列データで分類・予測させてみよう（RNN, LSTM）
## 解説
- RNN
  - 再帰型ニューラルネットワーク (Recurrent Neural Network; RNN) は、系列データを対象としたモデル
  - 系列データの持つ（時間的な）前後関係を踏まえた学習を目的としています。
  - RNNの入力データ
    - 系列データ
  - 各データは独立で処理されない
  - 時系列データに対して応用されます。
    - 株価の予測モデルや自然言語処理などへの適用が行われています。
- BPTTとRNNの注意点
  - 誤差逆伝播方で時間を遡って誤差を逆伝播させる
  - RNNの誤差逆伝播方はBackPropagation Through Time(BPTT)と呼んでいます。
  - 課題
    - 勾配爆発
      - 時間の分だけ勾配の積が発生するため、出力付近の勾配が過大
    - 勾配消失
      - 入力付近では勾配が過小になる傾向
## 精度向上Tips
- 勾配爆発
  - 対策
    - Gradient Clipping
      - 勾配の大きさそのものを制限してしまうという手法
      - あまりに制限値を小さくとると常に勾配が小さくなり、学習のスピードが落ちる点には注意
- ショートカットとゲートによる勾配消失への対処
  - 勾配消失
    - 対策
      - ショートカット
        - 各層の出力にその層への入力だったものも加えてしまうという手法
      - ゲート
        - ショートカットの一般化として重み付き和を考えるものです
- LSTM
  - ゲートの考え方を時間方向の隠れ状態の計算に用いる
  - 系列内の長期的な相互依存性をモデル化できるようにしたRNN
  - 時点 𝑡 の入力情報（時点 𝑡 の入力 𝑥𝑡 、前時点での出力 ℎ𝑡−1 を結合したもの）を入力ゲート 𝑖𝑡 を介して取り込んでいます。
  - 入力情報を用いて、前時点までの長期的な系列情報 𝑐𝑡−1 を 𝑐𝑡 に更新します。
  - このとき前時点までの情報 𝑐𝑡−1 をどれだけ重視するか、忘却ゲート 𝑓𝑡 が制御しています。
  - 最終的な出力は、時点tまでの系列情報 𝑐𝑡 を出力ゲート 𝑜𝑡 によって調整することで決定されます。
- GRU
  - ゲートの考え方を利用しながら、隠れ状態ベクトル ℎ𝑡 のみに長期の情報も集約したモデル
## 系列データで分類・予測させてみよう（RNN, LSTM）
- 時系列データのためトレーニングとテストデータはある時点区切る必要がある
-  ケーススタディ
   -  時系列データの異常検知
      -  予測とテストデータの差から、異常である可能性を算出しているというわけです。
   -  画像との組み合わせ

# ニューラル翻訳モデルを作ってみよう（Seq2Seq, Attention）
## 解説
- 言語モデル
  - 言語モデルとは、ある文章が生成される過程を確率的にモデル化したものです。
  - 文章を単語ごとに分割してならべ、その単語列が生成される確率をモデル化します。
  - 各単語の生成は、周囲の単語による条件付き確率分布によって決まると言えます。
  - ニューラル言語モデル
    - ニューラルネットワークを用いたもの
    - 順伝播型ニューラル言語モデル（FFNN言語モデル）：t番目の単語予想に直前の数単語分（固定長）だけ用いるモデルで、Denseレイヤーを用いる
    - 再帰型ニューラル言語モデル（RNN言語モデル）：t番目の単語予想にそれまでの系列全てを用いるモデルで、RNNやLSTMなどが相当する
- 単語のベクトル化と分散表現
  - 数値化
    - 単純な数値化として出現順や頻度順での番号割り振り
    - 、Kerasではkeras.preprocessing.text.Tokenizerクラスで簡単に実行可能
  - ベクトル化
    - one-hot表現
      - 単語同士の意味の近さが含まれない不便な表現
    - 分散表現
      - 単語同士の意味の近さを機械学習でモデル化したうえで、単語をベクトル化するもの
    - 埋め込み（Embedding）
      - これはone-hot表現よりも小さい次元に特徴量を圧縮してベクトル化しますが、この圧縮の仕方はニューラルネットワークで学習します。
    - word2vec
      - 分散表現の手法
- 系列変換モデル
  - ある文章を受けて、異なる文章を生成するようなモデル
  - 符号化器（左3ユニット）：
    - 入力系列を受け取って抽象化します
  - 復号化器（右5ユニット）
    - 抽象化された入力系列を加味しつつ、真の出力系列を元に各々1つ先の単語を出力します
  - Seq2Seqモデルを構成する場合には大まかに以下のようなレイヤー構成
    - 符号化器Embeddingレイヤー：特徴量変換（入力系列のone_hot表現→埋め込み表現）
    - 符号化器再帰レイヤー：入力系列を"抽象化"（最終的な隠れ状態ベクトルの取得が目的、符号化器の途中の出力系列には興味がない）
    - 復号化器Embeddingレイヤー：特徴量変換（(5で生成された)直前の出力単語のone_hot表現→埋め込み表現）
    - 復号化器再帰レイヤー：抽象化した入力系列を加味しながら（状態ベクトルの初期値として使う）、現在の単語の1つ先の単語を出力
    - 復号化器出力レイヤー：復号化器再帰レイヤーの出力系列をもとにして目的の出力系列に変換する（隠れ状態ベクトル表現→one-hot表現）
- Functional API
  - 途中に分岐や合流があるような複雑なモデルは作成する複雑なモデルの構築方法
  - 実装上の特徴は:
    - Inputレイヤーから構築を始める
    - 各レイヤーの返り値（テンソル）を次のレイヤーの入力として順々に構築していく
    - keras.models.Modelクラスに入力と出力を指定することでモデルを生成
## ニューラル翻訳モデルを作ってみよう（Seq2Seq, Attention）
  - 機械翻訳の評価について
  - BLEUスコア
    - 翻訳タスク特有の性質を反映した評価指標
    - n-gram（連続n単語. 主にn=4）がどれだけ生成文と正解文で共有されているかなどを考慮した指標
## 精度向上Tips
- Attention機構
  - 課題
    - 系列が長くなってしまえば、最初に符号化器に入力された情報が復号化器まで伝播することが期待できなくなっていきます。
  - Attention機構
    - 直接的に入力情報を出力時に利用する
    - 入力系列の中で各データ（単語）を重視すべき確率を計算し、この重視度合い（attention）を考慮して入力系列を参照します
    - soft attention：
      - attentionの分布による符号化器出力の期待値を復号化器出力に利用
      - 比較的実装が容易
      - スコア関数
        - 符号化器の状態を考慮値と符号化器の出力ベクトルとの関係を数値化し、どれを重視すればよいか測る
        - 1つの符号化器の隠れ状態ベクトルに対して、符号化器の系列長分だけのスコアを得る
        - 確率への変換
          - softmax関数を利用
        - 文脈ベクトル
          - Attentionによって、符号化器の隠れ状態ベクトルの重み付き平均
    - hard attention：
      - attentionの分布からのサンプリングによって選択した符号化器出力を復号化器出力に利用
## ケーススタディ
- 文書要約・対話システム
- 音声・画像への適用
- スケッチの自動描画

# 画像からキャプションを生成してみよう
## 解説
- キャプション生成とは
  - 画像を入力としてその画像の説明文（キャプション）を出力するタスク
  - 研究の意義
    - 例えば視覚障害のある人にWeb上にある画像の内容を理解させるために自動で説明文を生成する
  - 基本的なネットワーク構成
    - 画像の特徴量をCNNにより抽出し、その特徴量をもとにRNNで説明文を生成していく形
- 前Lesson（翻訳モデル）との関係性
  - 翻訳モデル
    - Encoder (RNN) で元言語文の特徴を抽出し、その抽出した特徴をもとにDecoder (RNN) でターゲット言語の文を生成していました
  - キャプション生成のモデルでも基本的な構造は同じ
  - 構成
    - CNNで画像の特徴を抽出し、その特徴をもとに説明文を生成します
  - 翻訳タスクとの違い
    - どのようにDecoder側の言語モデルの初期状態 (具体的にはRNNであれば最初の隠れ層 ℎ0 ) を条件付けるかという部分
    - 入力情報が画像となるので、EncoderをCNNに変更
- データセット

# ニューラルネットに画像を生成させよう
## 解説
- 深層生成モデル
  - 確率分布を深層ニューラルネットワークでモデル化するアプローチ
  - 代表的なモデル
    - Generative Adversarial Network (GAN)
  - 生成モデルの利用方法
    - サンプリング: 確率モデルを利用して未知のデータを生成出来る。
    - 密度推定: 外れ値検知や異常検知などに用いられる。
    - 欠損値補完: 欠損のあるデータを入力して真のデータの推定値が得られる。
- GAN
  - GANの概要
    - Generator
      - データセットと同じような画像を生成しようとします。 
    - Discriminator
      - 入力画像がデータセットの中にある本物の画像かGeneratorが生成した偽物の画像かどうかを判定します。
    - 短所
      - 学習が不安定なところ
    - 学習の進め方
      - Generator
        - Discrimnatorの学習とは独立に  𝑉(𝐷,𝐺)  を最小化するように学習します。
      - Discriminator
        - 学習においてGeneratorの学習とは独立に  𝑉(𝐷,𝐺)  を最大化するように学習します。
      - 学習を交互に行う
    - 実装のコツ
      - GANの収束性については現在も研究が進められていますが、依然として学習が不安定なので、ネットワークを設計する際は、論文のネットワークの形をそのまま利用したり、実際の実装コードを参考にしたりすると良いです。